--- a/mss_2013_10_31_33895/src/crypto/aesalgo.h
+++ b/mss_2013_10_31_33895/src/crypto/aesalgo.h
@@ -26,6 +26,7 @@
 #define MODE_CFB128             4         /* Are we ciphering in 128-bit CFB mode? */
 #define MODE_OFB                5         /* Are we ciphering in OFB mode? */
 
+MOC_EXTERN void aesNiInit(void);
 MOC_EXTERN sbyte4 aesKeySetupEnc(ubyte4 rk[/*4*(Nr + 1)*/], const ubyte cipherKey[], sbyte4 keyBits);
 MOC_EXTERN sbyte4 aesKeySetupDec(ubyte4 rk[/*4*(Nr + 1)*/], const ubyte cipherKey[], sbyte4 keyBits);
 MOC_EXTERN void aesEncrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte pt[16], ubyte ct[16]);
--- a/mss_2013_10_31_33895/src/crypto/aesalgo.c
+++ b/mss_2013_10_31_33895/src/crypto/aesalgo.c
@@ -9,6 +9,8 @@
  *
  */
 
+#define __VC_AESNI__	// undef to completely compile out AESNI code;
+
 #include "../common/moptions.h"
 #include "../common/mtypes.h"
 #include "../common/mocana.h"
@@ -750,16 +752,288 @@
     0x1B000000LU, 0x36000000LU, /* for 128-bit blocks, Rijndael never uses more than 10 rcon values */
 };
 
+/*------------------------------------------------------------------*/
+
+// init AESNI state;
+// determines if AESNI instructions are available on this platform;
+// cpuid aesni checked once for first aes context init;
+
+enum aesni_state {
+	AESNI_NOT_INIT = -1,
+	AESNI_NOT_AVAIL = 0,
+	AESNI_USE = 1,
+};
+
+int use_aesni = AESNI_NOT_INIT;
+
+extern void
+aesNiInit(void)
+{
+	register ubyte4 a, b, c, d;
+
+	a = 1;
+	use_aesni = AESNI_NOT_AVAIL;
+	asm volatile (
+		"cpuid"
+		: "=a" (a), "=b" (b), "=c" (c), "=d" (d)
+		: "a" (a)
+	);
+	if(c & (1 << 25))
+		use_aesni = AESNI_USE;
+}
 
 /*------------------------------------------------------------------*/
 
+#ifdef __VC_AESNI__
+
+// AESNI expand the encryption key into key schedule;
+// returns # of rounds, or 0 if error;
+// note: the expanded key u32 words are endian swapped compared to aesSwKeyEnc();
+
+extern sbyte4
+aesNiKeyEnc(ubyte4 rk[/*4*(Nr + 1)*/], const ubyte cipherKey[], sbyte4 keyBits)
+{
+	sbyte4 nr = 0;
+
+	if(keyBits == 128) {
+#define aeskeygen_xmm1_xmm2(imm8) ".byte 0x66, 0x0f, 0x3a, 0xdf, 0xd1, " #imm8 " \n\t"
+#define aeskey_expand128 \
+	"pshufd $0xff, %%xmm2, %%xmm2\n\t" \
+	"movdqu %%xmm1, %%xmm3\n\t" \
+	"pslldq $4, %%xmm3\n\t" \
+	"pxor   %%xmm3, %%xmm1\n\t" \
+	"pslldq $4, %%xmm3\n\t" \
+	"pxor   %%xmm3, %%xmm1\n\t" \
+	"pslldq $4, %%xmm3\n\t" \
+	"pxor   %%xmm3, %%xmm2\n\t" \
+	"pxor   %%xmm2, %%xmm1\n\t"
+
+	asm volatile (
+		"movdqu (%[key]), %%xmm1\n\t"
+		"movdqu %%xmm1, (%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x01)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x10(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x02)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x20(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x04)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x30(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x08)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x40(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x10)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x50(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x20)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x60(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x40)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x70(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x80)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x80(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x1b)
+		aeskey_expand128
+		"movdqu %%xmm1, 0x90(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x36)
+		aeskey_expand128
+		"movdqu %%xmm1, 0xa0(%[ksch])\n\t"
+		:
+		: [key] "r" (cipherKey),
+		  [ksch] "r" (rk)
+		: "cc", "memory"
+	);
+#undef aeskeygen_xmm1_xmm2
+#undef aeskey_expand128
+		nr = 10;
+	} else if(keyBits == 192) {
+#define aeskeygen_xmm3_xmm2(imm8) ".byte 0x66, 0x0f, 0x3a, 0xdf, 0xd3, " #imm8 " \n\t"
+#define aeskey_expand192 \
+	"pshufd $0x55, %%xmm2, %%xmm2\n\t" \
+	"movdqu %%xmm1, %%xmm4\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm1\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm1\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm1\n\t" \
+	"pxor %%xmm2, %%xmm1\n\t" \
+	"pshufd $0xff, %%xmm1, %%xmm2\n\t" \
+	"movdqu %%xmm3, %%xmm4\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm3\n\t" \
+	"pxor %%xmm2, %%xmm3\n\t"
+
+	asm volatile (
+		"movdqu (%[key]), %%xmm1\n\t"
+		"movq 16(%[key]), %%xmm3\n\t"
+		"movdqu %%xmm1, (%[ksch])\n\t"
+		"movdqu %%xmm3, %%xmm5\n\t"
+
+		aeskeygen_xmm3_xmm2(0x01)
+		aeskey_expand192
+		"shufpd $0, %%xmm1, %%xmm5\n\t"
+		"movdqu %%xmm5, 0x10(%[ksch])\n\t"
+		"movdqu %%xmm1, %%xmm6\n\t"
+		"shufpd $1, %%xmm3, %%xmm6\n\t"
+		"movdqu %%xmm6, 0x20(%[ksch])\n\t"
+		aeskeygen_xmm3_xmm2(0x02)
+		aeskey_expand192
+		"movdqu %%xmm1, 0x30(%[ksch])\n\t"
+		"movdqu %%xmm3, %%xmm5\n\t"
+
+		aeskeygen_xmm3_xmm2(0x04)
+		aeskey_expand192
+		"shufpd $0, %%xmm1, %%xmm5\n\t"
+		"movdqu %%xmm5, 0x40(%[ksch])\n\t"
+		"movdqu %%xmm1, %%xmm6\n\t"
+		"shufpd $1, %%xmm3, %%xmm6\n\t"
+		"movdqu %%xmm6, 0x50(%[ksch])\n\t"
+		aeskeygen_xmm3_xmm2(0x08)
+		aeskey_expand192
+		"movdqu %%xmm1, 0x60(%[ksch])\n\t"
+		"movdqu %%xmm3, %%xmm5\n\t"
+
+		aeskeygen_xmm3_xmm2(0x10)
+		aeskey_expand192
+		"shufpd $0, %%xmm1, %%xmm5\n\t"
+		"movdqu %%xmm5, 0x70(%[ksch])\n\t"
+		"movdqu %%xmm1, %%xmm6\n\t"
+		"shufpd $1, %%xmm3, %%xmm6\n\t"
+		"movdqu %%xmm6, 0x80(%[ksch])\n\t"
+		aeskeygen_xmm3_xmm2(0x20)
+		aeskey_expand192
+		"movdqu %%xmm1, 0x90(%[ksch])\n\t"
+		"movdqu %%xmm3, %%xmm5\n\t"
+
+		aeskeygen_xmm3_xmm2(0x40)
+		aeskey_expand192
+		"shufpd $0, %%xmm1, %%xmm5\n\t"
+		"movdqu %%xmm5, 0xa0(%[ksch])\n\t"
+		"movdqu %%xmm1, %%xmm6\n\t"
+		"shufpd $1, %%xmm3, %%xmm6\n\t"
+		"movdqu %%xmm6, 0xb0(%[ksch])\n\t"
+		aeskeygen_xmm3_xmm2(0x80)
+		aeskey_expand192
+		"movdqu %%xmm1, 0xc0(%[ksch])\n\t"
+		:
+		: [key] "r" (cipherKey),
+		  [ksch] "r" (rk)
+		: "cc", "memory"
+	);
+#undef aeskeygen_xmm3_xmm2
+#undef aeskey_expand192
+		nr = 12;
+	} else if(keyBits == 256) {
+#define aeskeygen_xmm1_xmm2(imm8) ".byte 0x66, 0x0f, 0x3a, 0xdf, 0xd1, " #imm8 " \n\t"
+#define aeskeygen_xmm3_xmm2(imm8) ".byte 0x66, 0x0f, 0x3a, 0xdf, 0xd3, " #imm8 " \n\t"
+#define aeskey_expand256a \
+	"pshufd $0xff, %%xmm2, %%xmm2\n\t" \
+	"movdqu %%xmm1, %%xmm4\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm1\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm1\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm1\n\t" \
+	"pxor %%xmm2, %%xmm1\n\t"
+#define aeskey_expand256b \
+	"pshufd $0xaa, %%xmm2, %%xmm2\n\t" \
+	"movdqu %%xmm3, %%xmm4\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm3\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm3\n\t" \
+	"pslldq $4, %%xmm4\n\t" \
+	"pxor %%xmm4, %%xmm3\n\t" \
+	"pxor %%xmm2, %%xmm3\n\t"
+
+	asm volatile ("movdqu (%[key]), %%xmm1\n\t"
+		"movdqu 16(%[key]), %%xmm3\n\t"
+		"movdqu %%xmm1, (%[ksch])\n\t"
+		"movdqu %%xmm3, 0x10(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x01)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0x20(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x00)
+		aeskey_expand256b
+		"movdqu %%xmm3, 0x30(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x02)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0x40(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x00)
+		aeskey_expand256b
+		"movdqu %%xmm3, 0x50(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x04)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0x60(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x00)
+		aeskey_expand256b
+		"movdqu %%xmm3, 0x70(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x08)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0x80(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x00)
+		aeskey_expand256b
+		"movdqu %%xmm3, 0x90(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x10)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0xa0(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x00)
+		aeskey_expand256b
+		"movdqu %%xmm3, 0xb0(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x20)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0xc0(%[ksch])\n\t"
+		aeskeygen_xmm1_xmm2(0x00)
+		aeskey_expand256b
+		"movdqu %%xmm3, 0xd0(%[ksch])\n\t"
+
+		aeskeygen_xmm3_xmm2(0x40)
+		aeskey_expand256a
+		"movdqu %%xmm1, 0xe0(%[ksch])\n\t"
+		:
+		: [key] "r" (cipherKey),
+		  [ksch] "r" (rk)
+		: "cc", "memory"
+	);
+#undef aeskeygen_xmm1_xmm2
+#undef aeskeygen_xmm3_xmm2
+#undef aeskey_expand256a
+#undef aeskey_expand256b
+		nr = 14;
+	}
+#define aeskey_cleanup() asm volatile ( \
+	"pxor %%xmm1, %%xmm1\n\t" \
+	"pxor %%xmm2, %%xmm2\n\t" \
+	"pxor %%xmm3, %%xmm3\n\t" \
+	"pxor %%xmm4, %%xmm4\n\t" \
+	"pxor %%xmm5, %%xmm5\n\t" \
+	"pxor %%xmm6, %%xmm6\n" \
+	::)
+
+	aeskey_cleanup();
+	return(nr);
+}
+
+#endif // __VC_AESNI__
+
 /**
  * Expand the cipher key into the encryption key schedule.
  *
  * @return  the number of rounds for the given cipher key size.
  */
 extern sbyte4
-aesKeySetupEnc(ubyte4 rk[/*4*(Nr + 1)*/], const ubyte cipherKey[], sbyte4 keyBits)
+aesSwKeyEnc(ubyte4 rk[/*4*(Nr + 1)*/], const ubyte cipherKey[], sbyte4 keyBits)
 {
 
 #ifdef __ENABLE_LLA_CAVIUM_HWXL__
@@ -893,9 +1167,83 @@
     return 0;
 }
 
+/**
+ * Expand the cipher key into the encryption key schedule.
+ *
+ * @return  the number of rounds for the given cipher key size.
+ */
+extern sbyte4
+aesKeySetupEnc(ubyte4 rk[/*4*(Nr + 1)*/], const ubyte cipherKey[], sbyte4 keyBits)
+{
+	sbyte4 (*func)(ubyte4 rk[], const ubyte key[], sbyte4 bits);
+
+	func = aesSwKeyEnc;
+#ifdef __VC_AESNI__
+	if(use_aesni == AESNI_USE)
+		func = aesNiKeyEnc;
+#endif // __VC_AESNI__
+	return(func(rk, cipherKey, keyBits));
+}
 
 /*------------------------------------------------------------------*/
 
+// invert the key for decryption;
+
+#ifdef __VC_AESNI__
+
+typedef struct { ubyte4 a, b, c, d; } u128_t;
+
+extern void
+aesNiKeyInv(void *dk, void *sk, sbyte4 nr)
+{
+	register u128_t *rk = dk;
+	register u128_t *ek = sk;
+
+#define aesimc_xmm1 ".byte 0x66, 0x0f, 0x38, 0xdb, 0xc9\n\t"
+#define aesimc_copy() asm volatile ( \
+	"movdqu %[ek], %%xmm1\n\t" \
+	"movdqu %%xmm1, %[rk]" \
+	: [rk] "=m" (*rk) \
+	: [ek] "m" (*ek) \
+	: "memory")
+#define aesimc_round(imc) asm volatile ( \
+	"movdqu %[ek], %%xmm1\n\t" \
+	aesimc_xmm1 \
+	"movdqu %%xmm1, %[rk]" \
+	: [rk] "=m" (*rk) \
+	: [ek] "m" (*ek) \
+	: "memory")
+#define aesimc_cleanup() asm volatile ( \
+	"pxor %%xmm1, %%xmm1\n" ::)
+
+	ek += nr;
+	aesimc_copy(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	aesimc_round(); rk++; ek--;
+	if(nr > 10) {
+		aesimc_round(); rk++; ek--;
+		aesimc_round(); rk++; ek--;
+		if(nr > 12) {
+			aesimc_round(); rk++; ek--;
+			aesimc_round(); rk++; ek--;
+		}
+	}
+	aesimc_copy();
+	aesimc_cleanup();
+#undef aesimc_xmm1
+#undef aesimc_round
+#undef aesimc_cleanup
+}
+
+#endif // __VC_AESNI__
+
 /**
  * Expand the cipher key into the decryption key schedule.
  *
@@ -943,8 +1291,7 @@
     sbyte4 Nr, i, j;
     ubyte4 temp;
 
-    /* expand the cipher key: */
-    Nr = aesKeySetupEnc(rk, cipherKey, keyBits);
+    Nr = (keyBits >> 5) + 6;
 
     /* invert the order of the round keys: */
     for (i = 0, j = 4*Nr; i < j; i += 4, j -= 4)
@@ -988,10 +1335,66 @@
 
 /*------------------------------------------------------------------*/
 
+#ifdef __VC_AESNI__
+
 extern void
-aesEncrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte pt[16], ubyte ct[16])
+aesNiEncrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte pt[16], ubyte ct[16])
 {
+#define aesenc_xmm1_xmm0      ".byte 0x66, 0x0f, 0x38, 0xdc, 0xc1\n\t"
+#define aesenclast_xmm1_xmm0  ".byte 0x66, 0x0f, 0x38, 0xdd, 0xc1\n\t"
+	asm volatile (
+		"movdqu %[src], %%xmm0\n\t"
+		"movdqu (%[key]), %%xmm1\n\t"
+		"pxor   %%xmm1, %%xmm0\n\t"
+		"movdqu 0x10(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x20(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x30(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x40(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x50(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x60(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x70(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x80(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0x90(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0xa0(%[key]), %%xmm1\n\t"
+		"cmpl $10, %[rounds]\n\t"
+		"jz .Lenclast%=\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0xb0(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0xc0(%[key]), %%xmm1\n\t"
+		"cmpl $12, %[rounds]\n\t"
+		"jz .Lenclast%=\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0xd0(%[key]), %%xmm1\n\t"
+		aesenc_xmm1_xmm0
+		"movdqu 0xe0(%[key]), %%xmm1\n"
+		".Lenclast%=:\n\t"
+		aesenclast_xmm1_xmm0
+		"movdqu %%xmm0, %[dst]\n"
+		: [dst] "=m" (*ct)
+		: [src] "m" (*pt),
+		  [key] "r" (rk),
+		  [rounds] "r" (Nr)
+		: "cc", "memory"
+	);
+#undef aesenc_xmm1_xmm0
+#undef aesenclast_xmm1_xmm0
+}
+
+#endif // __VC_AESNI__
 
+extern void
+aesSwEncrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte pt[16], ubyte ct[16])
+{
 #ifdef __ENABLE_LLA_CAVIUM_HWXL__
 
 
@@ -1151,14 +1554,80 @@
 
 }
 
+extern void
+aesEncrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte pt[16], ubyte ct[16])
+{
+	void (*func)(ubyte4 rk[], sbyte4 nr, ubyte pt[16], ubyte ct[16]);
+
+	func = aesSwEncrypt;
+#ifdef __VC_AESNI__
+	if(use_aesni == AESNI_USE)
+		func = aesNiEncrypt;
+#endif // __VC_AESNI__
+	func(rk, Nr, pt, ct);
+}
 
 /*------------------------------------------------------------------*/
 
+#ifdef __VC_AESNI__
+
 extern void
-aesDecrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte ct[16], ubyte pt[16])
+aesNiDecrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte ct[16], ubyte pt[16])
 {
+#define aesdec_xmm1_xmm0      ".byte 0x66, 0x0f, 0x38, 0xde, 0xc1\n\t"
+#define aesdeclast_xmm1_xmm0  ".byte 0x66, 0x0f, 0x38, 0xdf, 0xc1\n\t"
+	asm volatile ("movdqu %[src], %%xmm0\n\t"     /* xmm0 := *a     */
+		"movdqu (%[key]), %%xmm1\n\t"
+		"pxor   %%xmm1, %%xmm0\n\t"     /* xmm0 ^= key[0] */
+		"movdqu 0x10(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x20(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x30(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x40(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x50(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x60(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x70(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x80(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0x90(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0xa0(%[key]), %%xmm1\n\t"
+		"cmpl $10, %[rounds]\n\t"
+		"jz .Ldeclast%=\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0xb0(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0xc0(%[key]), %%xmm1\n\t"
+		"cmpl $12, %[rounds]\n\t"
+		"jz .Ldeclast%=\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0xd0(%[key]), %%xmm1\n\t"
+		aesdec_xmm1_xmm0
+		"movdqu 0xe0(%[key]), %%xmm1\n"
+		".Ldeclast%=:\n\t"
+		aesdeclast_xmm1_xmm0
+		"movdqu %%xmm0, %[dst]\n"
+		: [dst] "=m" (*pt)
+		: [src] "m" (*ct),
+		  [key] "r" (rk),
+		  [rounds] "r" (Nr)
+		: "cc", "memory"
+	);
+#undef aesdec_xmm1_xmm0
+#undef aesdeclast_xmm1_xmm0
+}
 
+#endif // __VC_AESNI__
 
+extern void
+aesSwDecrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte ct[16], ubyte pt[16])
+{
 #ifdef __ENABLE_LLA_CAVIUM_HWXL__
 
     uint64_t * iv1,iv2;
@@ -1310,6 +1779,18 @@
 
 }
 
+extern void
+aesDecrypt(ubyte4 rk[/*4*(Nr + 1)*/], sbyte4 Nr, ubyte ct[16], ubyte pt[16])
+{
+	void (*func)(ubyte4 rk[], sbyte4 nr, ubyte ct[16], ubyte pt[16]);
+
+	func = aesSwDecrypt;
+#ifdef __VC_AESNI__
+	if(use_aesni == AESNI_USE)
+		func = aesNiDecrypt;
+#endif // __VC_AESNI__
+	func(rk, Nr, ct, pt);
+}
 
 /*------------------------------------------------------------------*/
 
@@ -1345,17 +1826,26 @@
         encrypt = TRUE;
     }
 
-    if (encrypt)
-    {
-        pAesContext->Nr = aesKeySetupEnc(pAesContext->rk, keyMaterial, keyLen);
-    }
-    else
-    {
-        pAesContext->Nr = aesKeySetupDec(pAesContext->rk, keyMaterial, keyLen);
-    }
-
-    aesKeySetupEnc(pAesContext->ek, keyMaterial, keyLen);
+    // determine availability of AESNI on intel;
+    // key schedule expansion must happen before crypto ops;
 
+#ifdef __VC_AESNI__
+    if(use_aesni < AESNI_NOT_AVAIL)
+        aesNiInit();
+#endif // __VC_AESNI__
+
+    // expand the enc key schedule only once;
+
+    pAesContext->Nr = aesKeySetupEnc(pAesContext->ek, keyMaterial, keyLen);
+    MOC_MEMCPY(pAesContext->rk, pAesContext->ek, sizeof(pAesContext->ek));
+    if( !encrypt) {
+#ifdef __VC_AESNI__
+	if(use_aesni == AESNI_USE)
+        	aesNiKeyInv(pAesContext->rk, pAesContext->ek, pAesContext->Nr);
+	else
+#endif // __VC_AESNI__
+        	aesKeySetupDec(pAesContext->rk, keyMaterial, keyLen);
+    }
 
 exit:
     return status;
--- a/vc-aes-test/aes-test.c
+++ b/vc-aes-test/aes-test.c
@@ -0,0 +1,316 @@
+// aes speed test;
+// tests aesEncrypt() and aesDecrypt() functions;
+// to prove to ourselves that the aesni code works and is faster;
+
+#include <sys/types.h>
+#include <stdio.h>
+#include <stdlib.h>
+#include <stdint.h>
+#include <openssl/sha.h>
+
+#include "common/moptions.h"
+#include "common/mdefs.h"
+#include "common/mtypes.h"
+#include "common/merrors.h"
+#include "common/mstdlib.h"
+#include "common/mrtos.h"
+#include "ipsec/ipsec.h"
+#include "ipsec/ipsec_defs.h"
+#include "ipsec/ipsec_protos.h"
+#include "ipsec/ipsecconf.h"
+#include "ipsec/ipseckey.h"
+#include "crypto/hw_accel.h"
+#include "crypto/aesalgo.h"
+#include "crypto/aes.h"
+
+// have a large buffer for an all-cache hit and miss sweep;
+// sweep for 64,128,...,2048 byte blocks;
+// assume the max L3 is 4MB, so use 100x that for efficient misses;
+
+#define PBUF_SIZE 1024*1024*400
+
+u_char pbuf[PBUF_SIZE];
+
+// for random addresses use a table of random indicies;
+
+#define PKT_SIZE_MIN 64
+#define PKT_SIZE_MAX 2048
+#define MAX_PKT_IDX (PBUF_SIZE / PKT_SIZE_MIN)
+
+u_int pbuf_idx[MAX_PKT_IDX];
+
+// packet sizes for profile;
+// aes blocks are 16 byte, so should be multiple of that;
+
+u_int pkt_sizes[] = { 64, 128, 256, 512, 1024, 1600, 0 };
+
+// key sizes;
+
+u_int key_sizes[] = { 128, 192, 256, 0 };
+
+// buffer for round keys;
+
+u_char key[32];
+aesCipherContext aes_ctx;
+aesCipherContext *ctx = &aes_ctx;
+
+#define MODE_NONE 0
+
+// compute sha1 over block;
+// make printable;
+
+char *
+sha1_pkt(u_char *p, u_int len)
+{
+	char *hp;
+	u_int i;
+	u_char md[20];
+	static char hs[64];
+
+	SHA1(p, len, md);
+	for(hp = hs, i = 0; i < 20; i++, hp += 2)
+		sprintf(hp, "%02x", md[i]);
+	return(hs);
+}
+
+// dump key schedule;
+
+void
+dump_keys(void)
+{
+	int n;
+
+	for(n = 0; n < 4*15; n++) {
+		printf("%08x ", ctx->rk[n]);
+		if((n % 4) == 3)
+			printf("\n");
+	}
+}
+
+// dump first N packet hashes;
+
+void
+dump_pkt_sha1(u_int idx, u_int pkt_size, u_char *pt, u_char *ct)
+{
+	if(idx < 10) {
+		printf("  %s", sha1_pkt(pt, pkt_size));
+		printf(" %s\n", sha1_pkt(ct, pkt_size));
+	}
+}
+
+// stride tests;
+// both ciphertext and plaintext in linear order,
+// for maximum cache effect;
+
+// aes encrypt stride;
+
+void
+aes_inner_encrypt_stride(u_int ks, u_int pkt_size, u_int npkts)
+{
+	sbyte4 nr = (ks >> 5) + 6;
+	u_char *ct, *pt, *ep;
+	u_int n, off;
+	struct timeval t0, t1;
+	uint64_t dt;
+	double mbs;
+	char *hs;
+
+	AESALGO_makeAesKey(ctx, ks, key, 1, MODE_NONE);
+	//dump_keys();
+
+	pt = pbuf;
+	ct = pt + pkt_size;
+	ep = pbuf + (pkt_size * npkts);
+
+	gettimeofday(&t0, NULL);
+	for(n = 0; ct < ep; n++) {
+		for(off = 0; off < pkt_size; off += 16)
+			aesEncrypt(ctx->rk, nr, pt + off, ct + off);
+		//dump_pkt_sha1(n, pkt_size, pt, ct);
+		pt += (2 * pkt_size);
+		ct += (2 * pkt_size);
+	}
+	gettimeofday(&t1, NULL);
+
+	hs = sha1_pkt(pbuf, pkt_size * npkts);
+
+	dt = (t1.tv_sec * 1000000ULL) + t1.tv_usec
+		- (t0.tv_sec * 1000000ULL) - t0.tv_usec;
+	mbs = (double)(n * pkt_size) / (double)dt;
+	printf(" %s: k%u %u %ub packets in %lu usec, %f MB/s, %s\n",
+		__func__, ks, n, pkt_size, dt, mbs, hs);
+}
+
+// aes decrypt stride;
+
+void
+aes_inner_decrypt_stride(u_int ks, u_int pkt_size, u_int npkts)
+{
+	sbyte4 nr = (ks >> 5) + 6;
+	u_char *ct, *pt, *ep;
+	u_int n, off;
+	struct timeval t0, t1;
+	uint64_t dt;
+	double mbs;
+	char *hs;
+
+	AESALGO_makeAesKey(ctx, ks, key, 0, MODE_NONE);
+	//dump_keys();
+
+	pt = pbuf;
+	ct = pt + pkt_size;
+	ep = pbuf + (pkt_size * npkts);
+
+	gettimeofday(&t0, NULL);
+	for(n = 0; ct < ep; n++) {
+		for(off = 0; off < pkt_size; off += 16)
+			aesDecrypt(ctx->rk, nr, ct + off, pt + off);
+		//dump_pkt_sha1(n, pkt_size, pt, ct);
+		pt += (2 * pkt_size);
+		ct += (2 * pkt_size);
+	}
+	gettimeofday(&t1, NULL);
+
+	hs = sha1_pkt(pbuf, pkt_size * npkts);
+
+	dt = (t1.tv_sec * 1000000ULL) + t1.tv_usec
+		- (t0.tv_sec * 1000000ULL) - t0.tv_usec;
+	mbs = (double)(n * pkt_size) / (double)dt;
+	printf(" %s: k%u %u %ub packets in %lu usec, %f MB/s, %s\n",
+		__func__, ks, n, pkt_size, dt, mbs, hs);
+}
+
+// random tests;
+// both ciphertext and plaintext in random order,
+// for maximum cache misses;
+
+// aes encrypt stride;
+
+void
+aes_inner_encrypt_random(u_int ks, u_int pkt_size, u_int npkts)
+{
+	sbyte4 nr = (ks >> 5) + 6;
+	u_int *idxp, *ep;
+	u_char *ct, *pt;
+	u_int n, off;
+	struct timeval t0, t1;
+	uint64_t dt;
+	double mbs;
+	char *hs;
+
+	AESALGO_makeAesKey(ctx, ks, key, 1, MODE_NONE);
+	//dump_keys();
+
+	idxp = pbuf_idx;
+	ep = idxp + npkts;
+
+	gettimeofday(&t0, NULL);
+	for(n = 0; idxp < ep; n++, idxp += 2) {
+		pt = pbuf + idxp[0];
+		ct = pbuf + idxp[1];
+		for(off = 0; off < pkt_size; off += 16)
+			aesEncrypt(ctx->rk, nr, pt + off, ct + off);
+	}
+	gettimeofday(&t1, NULL);
+
+	hs = sha1_pkt(pbuf, pkt_size * npkts);
+
+	dt = (t1.tv_sec * 1000000ULL) + t1.tv_usec
+		- (t0.tv_sec * 1000000ULL) - t0.tv_usec;
+	mbs = (double)(n * pkt_size) / (double)dt;
+	printf(" %s: k%u %u %ub packets in %lu usec, %f MB/s, %s\n",
+		__func__, ks, n, pkt_size, dt, mbs, hs);
+}
+
+// aes decrypt stride;
+
+void
+aes_inner_decrypt_random(u_int ks, u_int pkt_size, u_int npkts)
+{
+	sbyte4 nr = (ks >> 5) + 6;
+	u_int *idxp, *ep;
+	u_char *ct, *pt;
+	u_int n, off;
+	struct timeval t0, t1;
+	uint64_t dt;
+	double mbs;
+	char *hs;
+
+	AESALGO_makeAesKey(ctx, ks, key, 0, MODE_NONE);
+	//dump_keys();
+
+	idxp = pbuf_idx;
+	ep = idxp + npkts;
+
+	gettimeofday(&t0, NULL);
+	for(n = 0; idxp < ep; n++, idxp += 2) {
+		pt = pbuf + idxp[0];
+		ct = pbuf + idxp[1];
+		for(off = 0; off < pkt_size; off += 16)
+			aesDecrypt(ctx->rk, nr, ct + off, pt + off);
+	}
+	gettimeofday(&t1, NULL);
+
+	hs = sha1_pkt(pbuf, pkt_size * npkts);
+
+	dt = (t1.tv_sec * 1000000ULL) + t1.tv_usec
+		- (t0.tv_sec * 1000000ULL) - t0.tv_usec;
+	mbs = (double)(n * pkt_size) / (double)dt;
+	printf(" %s: k%u %u %ub packets in %lu usec, %f MB/s, %s\n",
+		__func__, ks, n, pkt_size, dt, mbs, hs);
+}
+
+// main entry;
+
+int
+main(int argc, char **argv)
+{
+	u_char *p;
+	u_int *idxp, *sizep;
+	u_int pkt_size, npkts;
+	u_int *ksp, ks;
+
+	// get random seed;
+
+	srandom(0x1);
+
+	// fill buffer with randoms;
+	// randoms will encrypt/decrypt into other randoms;
+	// we shouldn't start with a 0-fill buffer, as code could have holes;
+
+	printf("filling buffer randoms\n");
+	for(p = pbuf; p < pbuf + sizeof(pbuf); p += 4)
+		*((u_int *)p) = random();
+
+	// run tests for all the pkt sizes;
+
+	printf("starting tests\n");
+	for(sizep = pkt_sizes; pkt_size = *sizep; sizep++) {
+		npkts = sizeof(pbuf) / pkt_size;
+
+		// make new random key;
+
+		printf("filling random key\n");
+		for(p = key; p < key + 32; p++)
+			*p = random();
+
+		// make new pkt indicies for each test;
+
+		for(idxp = pbuf_idx; idxp < pbuf_idx + npkts; idxp++)
+			*idxp = random() % npkts;
+
+		// run tests for each key size;
+
+		for(ksp = key_sizes; ks = *ksp; ksp++) {
+
+			aes_inner_encrypt_stride(ks, pkt_size, npkts);
+			aes_inner_decrypt_stride(ks, pkt_size, npkts);
+
+			aes_inner_encrypt_random(ks, pkt_size, npkts);
+			aes_inner_decrypt_random(ks, pkt_size, npkts);
+		}
+	}
+
+	return(0);
+}
+
--- a/vc-aes-test/Makefile
+++ b/vc-aes-test/Makefile
@@ -0,0 +1,16 @@
+
+MOC = ../mss_2013_10_31_33895
+
+INCL = -I$(MOC)/src
+
+DEFS += -D__RTOS_LINUX__
+DEFS += -D__ENABLE_MOCANA_64_BIT__=
+DEFS += -D__IKE_MULTI_HOMING__=
+DEFS += -D__ENABLE_IPSEC_NAT_T__=
+DEFS += -D__ENABLE_IPSEC_COOKIE__=
+
+LIBS = -lcrypto $(MOC)/bin/libikeipsec.a
+
+aes-test: aes-test.c
+	cc -o $@ -O $(DEFS) $(INCL) $< $(LIBS)
+
